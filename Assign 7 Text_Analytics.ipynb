{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Text Analytics\n",
        "1. Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
        "Frequency.\n",
        "\n",
        ".\n",
        ".\n",
        "\n",
        "In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)"
      ],
      "metadata": {
        "id": "kj5kvEGBnsxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xFxnABeKuQqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ3tweSBnhII",
        "outputId": "b159e812-fb09-4d20-a6d6-06f509e501eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizations\n",
        "\n",
        "**Word tokenize: We use the word_tokenize() method to split a sentence into tokens or words. Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences.**\n",
        "\n"
      ],
      "metadata": {
        "id": "27Z-5YAQoe5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to split text into word\n",
        "from nltk.tokenize import word_tokenize\n",
        "w1 = word_tokenize(\"Hello My name is Mayur.\")\n",
        "print(w1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tie8clFDoG_0",
        "outputId": "ecaf4961-0e16-4561-c627-57d299bd5086"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'My', 'name', 'is', 'Mayur', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "w2 = sent_tokenize(\"hello my name is mayur. I am computer engineering student.\")\n",
        "print(w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYtJgSA_p5FQ",
        "outputId": "6f66f8a2-bed2-43cf-b982-512b3b56ee8c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello my name is mayur.', 'I am computer engineering student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS (Part of Speech) Tagging\n",
        "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words. \n"
      ],
      "metadata": {
        "id": "azpLgCBsrNjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIKEuP8LrFst",
        "outputId": "0f099120-ec77-47c3-d4d0-6110929eecdb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Are you afraid of something?\"\n",
        "word = word_tokenize(text)\n",
        "pos_tag(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBJ5TDC2rsTv",
        "outputId": "a30b8486-9485-4fa9-e5bf-7a1e5af5b3d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Are', 'NNP'),\n",
              " ('you', 'PRP'),\n",
              " ('afraid', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword removal\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc."
      ],
      "metadata": {
        "id": "s8_1Y6AqsSBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop1 = stopwords.words('english')\n",
        "print(stop1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snOYlpfHsJ6C",
        "outputId": "acf6f057-013f-4549-b54f-e801aa5e9bf1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stop1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OBnJqVosxTs",
        "outputId": "b683de93-0243-40dd-bdd0-96ca3d0479fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = 'i like mathematics. mathematics is the easiest subject in my life'\n",
        "clean_text = []\n",
        "word = word_tokenize(txt)\n",
        "for w in word:\n",
        "  if w not in stop1:\n",
        "    clean_text.append(w)\n",
        "\n",
        "print(\"Original text\",word)\n",
        "print(\"after stop word removal\", clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clH0NoqAtAZS",
        "outputId": "b72ae79c-eebf-4658-c8a5-7c6185a1f181"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text ['i', 'like', 'mathematics', '.', 'mathematics', 'is', 'the', 'easiest', 'subject', 'in', 'my', 'life']\n",
            "after stop word removal ['like', 'mathematics', '.', 'mathematics', 'easiest', 'subject', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling."
      ],
      "metadata": {
        "id": "FHGTU057tzCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import SnowballStemmer\n",
        "sbs = SnowballStemmer('english')\n",
        "text = \"Nltk full form is natural language took kit. Engineering needs a vision\"\n",
        "word = word_tokenize(text)\n",
        "print(\"Original Word: Word after stemming\")\n",
        "for w in word:\n",
        "  print(w, \" : \", sbs.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egYrvixxvFAR",
        "outputId": "dcdd200e-dfa0-4c0b-e595-3ecd019b704b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Word: Word after stemming\n",
            "Nltk  :  nltk\n",
            "full  :  full\n",
            "form  :  form\n",
            "is  :  is\n",
            "natural  :  natur\n",
            "language  :  languag\n",
            "took  :  took\n",
            "kit  :  kit\n",
            ".  :  .\n",
            "Engineering  :  engin\n",
            "needs  :  need\n",
            "a  :  a\n",
            "vision  :  vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma."
      ],
      "metadata": {
        "id": "opxf1zIVvvph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "wlm = WordNetLemmatizer()\n",
        "word = ['give', 'giving', 'leaves', 'gave']\n",
        "for w in word: \n",
        "  print(wlm.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpw_JxZwvz1x",
        "outputId": "144a1880-6bdb-4d65-c489-b77671a64dec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give\n",
            "giving\n",
            "leaf\n",
            "gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency)**, a commonly used weighting technique for information retrieval and information exploration.\n",
        "\n",
        "TF-IDF is a statistical method used to evaluate the importance of a word to a file set or a file in a corpus. The importance of the word increases in proportion to the number of times it appears in the file, but at the same time decreases inversely with the frequency of its appearance in the corpus.\n",
        "\n",
        "* **Term frequency TF (item frequency)**: number of times a given word appears in the text. This number is usually normalized (the numerator is generally smaller than the denominator) to prevent it from favoring long documents, because whether the term is important or not, it is likely to appear more often in long documents than in paragraph documents.\n",
        "\n",
        "> **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
        "\n",
        "Term frequency (TF) indicates how often a term (keyword) appears in the text .\n",
        "\n",
        "This number is usually normalized (usually the word frequency divided by the total number of words in the article) to prevent it from favoring long documents.\n",
        "\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Consider a document containing 100 words where in the word cat appears 3 times. \n",
        "\n",
        "The **term frequency (Tf) for cat** is then **(3 / 100) = 0.03**. Now, assume we have 10 million documents and the word cat appears in one thousand of these.\n",
        "\n",
        "Then, the **inverse document frequency (Idf)** is calculated as **log(10,000,000 / 1,000) = 4.** \n",
        "\n",
        "Thus, the **Tf-idf** weight is the product of these quantities: **0.03 * 4 = 0.12.**"
      ],
      "metadata": {
        "id": "MaGRLiUYwRfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "d0 = 'new york times'\n",
        "d1 = 'new york post'\n",
        "d2 = 'los angels time'\n",
        "series = [d0,d1,d2]\n"
      ],
      "metadata": {
        "id": "oZnloHy7wYCJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an object of tf-idf\n",
        "tf_idf = TfidfVectorizer()\n",
        "#get tf-idf values\n",
        "result = tf_idf.fit_transform(series)"
      ],
      "metadata": {
        "id": "E4iDkbA3wweL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUPX8HkrxApf",
        "outputId": "bbd1e118-f2bc-4cb4-c634-7fa35cbc32e4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5)\t0.680918560398684\n",
            "  (0, 6)\t0.5178561161676974\n",
            "  (0, 2)\t0.5178561161676974\n",
            "  (1, 3)\t0.680918560398684\n",
            "  (1, 6)\t0.5178561161676974\n",
            "  (1, 2)\t0.5178561161676974\n",
            "  (2, 4)\t0.5773502691896257\n",
            "  (2, 0)\t0.5773502691896257\n",
            "  (2, 1)\t0.5773502691896257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import TextCollection\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sents = ['this is sentence one', 'this is sentence two', 'this is sentence three']\n",
        "\n",
        "sents = [word_tokenize(sent) for sent in sents]\n",
        "\n",
        "print(sents)\n",
        "\n",
        "cps = TextCollection(sents)\n",
        "print(cps)\n",
        "\n",
        "tf=cps.tf('one', cps)\n",
        "print(tf)\n",
        "\n",
        "idf=cps.idf('one')\n",
        "print(idf)\n",
        "\n",
        "tf_idf=cps.tf_idf('one',cps)\n",
        "print(tf_idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53jhpynP1The",
        "outputId": "c245cc58-a617-407d-8028-5e97abc39937"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['this', 'is', 'sentence', 'one'], ['this', 'is', 'sentence', 'two'], ['this', 'is', 'sentence', 'three']]\n",
            "<Text: this is sentence one this is sentence two...>\n",
            "0.08333333333333333\n",
            "1.0986122886681098\n",
            "0.0915510240556758\n"
          ]
        }
      ]
    }
  ]
}