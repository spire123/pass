{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ed005c",
   "metadata": {},
   "source": [
    "# Cheet Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca29fe",
   "metadata": {},
   "source": [
    "Data tables are presented in Comma Delimited, CSV text file format. Although this file format allows for the data table to be easily retrieved into a variety of applications, they are best viewed within one that will allow one to easily manipulate data that is in columnar format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"autodata.csv\")\n",
    "df.head()\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285021a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stroke = df['stroke'].astype('float').mean(axis=0)\n",
    "print(avg_stroke)\n",
    "\n",
    "df['stroke'].replace(np.nan, avg_stroke, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778df3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num-of-doors'].value_counts()\n",
    "df['num-of-doors'].value_counts().idxmax()\n",
    "#drop row with null values in perticular column\n",
    "df.dropna(subset=['horsepower-binned'], axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaging the data values data standardization\n",
    "df['city-L/100km'] = 235/df[\"city-mpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf72739",
   "metadata": {},
   "source": [
    "**Data Wrangling is the process of converting data from the initial format to a format that may be better for analysis.**\n",
    "\n",
    "**Data Normalization**\n",
    "Normalization is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling variable so the variable values range from 0 to 1\n",
    "\n",
    "**Data Standardization**\n",
    "Data is usually collected from different agencies with different formats. (Data Standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation)\n",
    "\n",
    "**What is Standardization?**\n",
    "\n",
    "Standardization is the process of transforming data into a common format which allows the researcher to make the meaningful comparison.\n",
    "\n",
    "**Evaluating for Missing Data**\n",
    "The missing values are converted to Python's default. We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:\n",
    "\n",
    "1. isnull()\n",
    "2. notnull()\n",
    "The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data. \"True\" stands for missing value, while \"False\" stands for not missing value.\n",
    "\n",
    "Deal with missing data\n",
    "\n",
    "Drop data\n",
    "Drop the whole row\n",
    "Drop the whole column\n",
    "Replace data\n",
    "Replace it by mean\n",
    "Replace it by frequency / mode\n",
    "Replace it based on other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6389b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['length']/df['length'].max()\n",
    "df[['length', 'width']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['Maths'])\n",
    "df.skew(axis=0) #col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba9058",
   "metadata": {},
   "source": [
    "1. Mean: The arithmetical mean is the sum of a set of numbers separated by the number of numbers in the collection, or simply the mean or the average.\n",
    "\n",
    "2. Median: In a sorted, ascending or descending, list of numbers, the median is the middle number and may be more representative of that data set than the average.\n",
    "\n",
    "3. Mode: The mode is the value that most frequently appears in a data value set.\n",
    "\n",
    "4. Standard Deviation: A calculation of the amount of variance or dispersion of a set of values is the standard deviation.\n",
    "\n",
    "5. Variance: The expectation of the square deviation of a random variable from its mean is variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique(axis=0) #cols\n",
    "df.nunique(axis=1) #rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (df[\"Marks\"]<35).any():\n",
    "    print(df[df['Marks']<35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ccff9f",
   "metadata": {},
   "source": [
    "**Assign 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fef90",
   "metadata": {},
   "source": [
    "Descriptive statistics are brief informational coefficients that summarize a given data set, which can be either a representation of the entire population or a sample of a population. Descriptive statistics are broken down into measures of central tendency and measures of variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f2f22",
   "metadata": {},
   "source": [
    "Measure of central tendency shows where the center or middle of the data set is located, whereas measure of variation shows the dispersion among data values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d30e32",
   "metadata": {},
   "source": [
    "1. Variance: A measure of how far a set of numbers are spread out from each other. It describes how far the numbers lie from the mean (expected value). It is the square of standard deviation.\n",
    "2. Standard deviation (SD): it is only used for data that are “normally distributed”. SD indicates how much a set of values is spread around the average. SD is determined by the variance (SD=the root of the variance).\n",
    "3. Interquartile range (IQR): the interquartile range (IQR), is also known as the 'midspread' or 'middle fifty', is a measure of statistical dispersion, being equal to the difference between the third and first quartiles[3]. IQR = Q3 − Q1. Unlike (total) range, the interquartile range is a more commonly used statistic, since it excludes the lower 25% and upper 25%, therefore reflecting more accurately valid values and excluding the outliers.\n",
    "4. Range: it is the length of the smallest interval which contains all the data and is calculated by subtracting the smallest observation (sample minimum) from the greatest (sample maximum) and provides an indication of statistical dispersion. It bears the same units as the data used for calculating it. Because of its dependance on just two observations, it tends to be a poor and weak measure of dispersion, with the only exception being when the sample size is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced547cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[:, [1,5]])\n",
    "print(df.iloc[:, 1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter queartile range\n",
    "from scipy.stats import iqr\n",
    "iqr(df['ApplicantIncome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6d53a",
   "metadata": {},
   "source": [
    "The skewness values can be interpreted in the following manner:\n",
    "1. Highly skewed distribution: If the skewness value is less than −1 or greater than +1.\n",
    "2. Moderately skewed distribution: If the skewness value is between −1 and −½ or between +½ and +1.\n",
    "3. Approximately symmetric distribution: If the skewness value is between −½ and +½."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0:'Sepal length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Species').mean()\n",
    "df['Species'].value_counts()\n",
    "df.groupby(['Species']).count()\n",
    "df['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=boston.data, columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81d5ab",
   "metadata": {},
   "source": [
    "**Assign 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb50282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rsme = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(\"Root Mean Square Error is: \")\n",
    "print(rsme)\n",
    "\n",
    "print(\"Accuracy of Training is: \")\n",
    "lr.score(x_train,y_train)\n",
    "\n",
    "print(\"Accuracy of Testing is: \")\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde27be",
   "metadata": {},
   "source": [
    "With random_state=0 , we get the same train and test sets across different executions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68f4a3",
   "metadata": {},
   "source": [
    "Linear regression analysis is used to predict the value of a variable based on the value of another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61568e2",
   "metadata": {},
   "source": [
    "The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786fd1f",
   "metadata": {},
   "source": [
    "Standard scalar standardizes features of the data set by scaling to unit variance and removing the mean (optionally) using column summary statistics on the samples in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bccd1",
   "metadata": {},
   "source": [
    "Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "\n",
    "y_predict = classifier.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,precision_score, recall_score, mean_squared_error\n",
    "cm = confusion_matrix(y_test,y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48366ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_predict)\n",
    "\n",
    "recall_score(y_test,y_predict)\n",
    "\n",
    "f1_score(y_test,y_predict)\n",
    "\n",
    "precision_score(y_test,y_predict)\n",
    "\n",
    "mean_squared_error(y_test,y_predict)\n",
    "\n",
    "classification_report(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3878a0f",
   "metadata": {},
   "source": [
    "#### Bayes' Theorem:\n",
    "\n",
    "Bayes' theorem is also known as Bayes' Rule or Bayes' law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.\n",
    "\n",
    "It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features.\n",
    "\n",
    " **P(A|B) = [P(B|A)P(A)] / P(B)**\n",
    "\n",
    "Where,\n",
    "\n",
    "P(A|B) is Posterior probability: Probability of hypothesis A on the observed event B.\n",
    "\n",
    "P(B|A) is Likelihood probability: Probability of the evidence given that the probability of a hypothesis is true.\n",
    "\n",
    "P(A) is Prior Probability: Probability of hypothesis before observing the evidence.\n",
    "\n",
    "P(B) is Marginal Probability: Probability of Evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "print(cm)\n",
    "disp.plot()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78474b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix_values(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])\n",
    "\n",
    "TP, FP, FN, TN = get_confusion_matrix_values(y_test, y_pred)\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FN: \", FN)\n",
    "\n",
    "print(\"The Accuracy is: \", (TP+TN)/(TP+TN+FP+FN))\n",
    "print(\"The precision is: \", TP/(TP+FP))\n",
    "print(\"The recall is: \", TP/(TP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1746d",
   "metadata": {},
   "source": [
    "Confusion matrices have two types of errors: Type I and Type II\n",
    "\n",
    "1. Accuracy (all correct / all) = TP + TN / TP + TN + FP + FN\n",
    "2. Misclassification (all incorrect / all) = FP + FN / TP + TN + FP + FN\n",
    "3. Precision (true positives / predicted positives) = TP / TP + FP\n",
    "4. Sensitivity aka Recall (true positives / all actual positives) = TP / TP + FN\n",
    "5. Specificity (true negatives / all actual negatives) =TN / TN + FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c5220",
   "metadata": {},
   "source": [
    "**Assign 7**\n",
    "\n",
    "In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9cd77",
   "metadata": {},
   "source": [
    "Tokenizations\n",
    "Word tokenize: We use the word_tokenize() method to split a sentence into tokens or words. Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a96ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "w1 = word_tokenize(\"hello i am yuuurlrl akdkd\")\n",
    "\n",
    "print(w1)\n",
    "form nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fe389",
   "metadata": {},
   "source": [
    "POS (Part of Speech) Tagging\n",
    "\n",
    "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c16afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'alslkkjdk dkdk'\n",
    "word = word_tokenize(text)\n",
    "pos_tag(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15042f6",
   "metadata": {},
   "source": [
    "Stopword removal\n",
    "\n",
    "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop1 = stopwords.words('english')\n",
    "\n",
    "txt = 'i like mathematics. mathematics is the easiest subject in my life'\n",
    "clean_text = []\n",
    "word = word_tokenize(txt)\n",
    "for w in word:\n",
    "  if w not in stop1:\n",
    "    clean_text.append(w)\n",
    "\n",
    "print(\"Original text\",word)\n",
    "print(\"after stop word removal\", clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbb817",
   "metadata": {},
   "source": [
    "Stemming\n",
    "Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "sbs = SnowballStemmer('english')\n",
    "text = \"Nltk full form is natural language took kit. Engineering needs a vision\"\n",
    "word = word_tokenize(text)\n",
    "print(\"Original Word: Word after stemming\")\n",
    "for w in word:\n",
    "  print(w, \" : \", sbs.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fc95f",
   "metadata": {},
   "source": [
    "Lemmatization\n",
    "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d66f48e",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**, a commonly used weighting technique for information retrieval and information exploration.\n",
    "\n",
    "TF-IDF is a statistical method used to evaluate the importance of a word to a file set or a file in a corpus. The importance of the word increases in proportion to the number of times it appears in the file, but at the same time decreases inversely with the frequency of its appearance in the corpus.\n",
    "\n",
    "* **Term frequency TF (item frequency)**: number of times a given word appears in the text. This number is usually normalized (the numerator is generally smaller than the denominator) to prevent it from favoring long documents, because whether the term is important or not, it is likely to appear more often in long documents than in paragraph documents.\n",
    "\n",
    "> **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
    "\n",
    "Term frequency (TF) indicates how often a term (keyword) appears in the text .\n",
    "\n",
    "This number is usually normalized (usually the word frequency divided by the total number of words in the article) to prevent it from favoring long documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd54f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "d0 = 'new york times'\n",
    "d1 = 'new york post'\n",
    "d2 = 'los angels time'\n",
    "series = [d0,d1,d2]\n",
    "\n",
    "#Create an object of tf-idf\n",
    "tf_idf = TfidfVectorizer()\n",
    "#get tf-idf values\n",
    "result = tf_idf.fit_transform(series)\n",
    "\n",
    "from nltk.text import TextCollection\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sents = ['this is sentence one', 'this is sentence two', 'this is sentence three']\n",
    "\n",
    "sents = [word_tokenize(sent) for sent in sents]\n",
    "\n",
    "print(sents)\n",
    "\n",
    "cps = TextCollection(sents)\n",
    "print(cps)\n",
    "\n",
    "tf=cps.tf('one', cps)\n",
    "print(tf)\n",
    "\n",
    "idf=cps.idf('one')\n",
    "print(idf)\n",
    "\n",
    "tf_idf=cps.tf_idf('one',cps)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=data['survived'])\n",
    "sns.boxplot(x=data['age'])\n",
    "sns.histplot(x=data['fare'], data=data, bins=20, hue='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['sex'], y=data['male'], color='red')\n",
    "myplt = {\"male\":\"b\", \"female\":\"r\"}\n",
    "sns.boxplot(x=data['sex'], y=data['age'], palette=myplt)\n",
    "sns.boxplot(x=data['sex'], y=data['age'], data=data, hue='survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5aae7e",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. We created a box plot of variables 'age & 'sex' & used survival as the hue\n",
    "2. The we visualized three variables Age, Sex & Survival. Two out of these are categorical and one is numerica\n",
    "3. The median age of female who didn't survived is slightly lower than female survived.\n",
    "4. The median age of male who didn't survived is slightly greater than male survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f142b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=data[\"SepalLengthCm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.iloc[:,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf79384",
   "metadata": {},
   "outputs": [],
   "source": [
    "yplt = {\"Iris-setosa\":\"b\", \"Iris-versicolor\":\"r\", \"Iris-virginica\":\"y\"}\n",
    "sns.boxplot(x=data['Species'], y=data[\"SepalLengthCm\"], data=data, palette=myplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=data['Species'], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data['SepalWidthCm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5df690",
   "metadata": {},
   "source": [
    "A histogram is a graphical representation of data points organized into user-specified ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861fcc7",
   "metadata": {},
   "source": [
    "A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. Heatmaps are used in various forms of analytics but are most commonly used to show user behavior on specific webpages or webpage templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88851f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
